{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caaee8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import struct\n",
    "\n",
    "# --- Data Structures ---\n",
    "@struct.dataclass\n",
    "class Transition:\n",
    "    obs: jax.Array\n",
    "    act: jax.Array\n",
    "    rew: jax.Array\n",
    "    next_obs: jax.Array\n",
    "    done: jax.Array \n",
    "\n",
    "@struct.dataclass\n",
    "class BufferState:\n",
    "    data: Transition\n",
    "    ptr: jax.Array      # Shape (NumEnvs,)\n",
    "    count: jax.Array    # Shape (NumEnvs,)\n",
    "    capacity: int\n",
    "\n",
    "# --- The Logic ---\n",
    "class BatchedReplayBuffer:\n",
    "    def __init__(self, num_envs, capacity, obs_dim, act_dim):\n",
    "        self.num_envs = num_envs\n",
    "        self.capacity = capacity\n",
    "        # We pre-allocate the memory on GPU\n",
    "        self.obs_shape = (num_envs, capacity, obs_dim)\n",
    "        self.act_shape = (num_envs, capacity, act_dim)\n",
    "        self.scalar_shape = (num_envs, capacity)\n",
    "\n",
    "    def init(self):\n",
    "        return BufferState(\n",
    "            data=Transition(\n",
    "                obs=jnp.zeros(self.obs_shape),\n",
    "                act=jnp.zeros(self.act_shape),\n",
    "                rew=jnp.zeros(self.scalar_shape),\n",
    "                next_obs=jnp.zeros(self.obs_shape),\n",
    "                done=jnp.zeros(self.scalar_shape),\n",
    "            ),\n",
    "            ptr=jnp.zeros(self.num_envs, dtype=jnp.int32),\n",
    "            count=jnp.zeros(self.num_envs, dtype=jnp.int32),\n",
    "            capacity=self.capacity\n",
    "        )\n",
    "\n",
    "    def add(self, state: BufferState, batch: Transition):\n",
    "        # 1. Expand data to broadcast correctly: (NumEnvs, 1, Dim)\n",
    "        # '...' detects remaining dims, making this part universal\n",
    "        def fix(x): return x[:, None, ...] if x.ndim > 1 else x[:, None]\n",
    "        \n",
    "        # 2. Use vmap to insert into each env's circular buffer\n",
    "        # This is the \"Independent Buffer\" logic        \n",
    "        def insert_slice(buf, update, p):\n",
    "            return jax.lax.dynamic_update_slice_in_dim(buf, update, start_index=p, axis=0)\n",
    "\n",
    "        # We vmap over the \"Environment\" axis (0)\n",
    "        new_data = Transition(\n",
    "            obs      = jax.vmap(insert_slice)(state.data.obs     , fix(batch.obs)     , state.ptr),\n",
    "            act      = jax.vmap(insert_slice)(state.data.act     , fix(batch.act)     , state.ptr),\n",
    "            rew      = jax.vmap(insert_slice)(state.data.rew     , fix(batch.rew)     , state.ptr),\n",
    "            next_obs = jax.vmap(insert_slice)(state.data.next_obs, fix(batch.next_obs), state.ptr),\n",
    "            done     = jax.vmap(insert_slice)(state.data.done    , fix(batch.done)    , state.ptr)\n",
    "        )\n",
    "\n",
    "        new_ptr = (state.ptr + 1) % self.capacity\n",
    "        new_count = jnp.minimum(state.count + 1, self.capacity)\n",
    "        return state.replace(data=new_data, ptr=new_ptr, count=new_count)\n",
    "\n",
    "    def sample(self, state: BufferState, rng, batch_size):\n",
    "        k1, k2 = jax.random.split(rng)\n",
    "        \n",
    "        # 1. Randomly pick WHICH environments to sample from\n",
    "        # Shape: (batch_size,)\n",
    "        env_indices = jax.random.randint(k1, (batch_size,), 0, self.num_envs)\n",
    "        \n",
    "        # 2. Lookup the SPECIFIC count for each chosen environment\n",
    "        # We use the indices from step 1 to grab the correct 'count' for that env.\n",
    "        # Shape: (batch_size,) e.g. [20, 5, 100, 20, ...]\n",
    "        batch_counts = state.count[env_indices]\n",
    "        \n",
    "        # 3. Sample time indices using the SPECIFIC bounds\n",
    "        # jax.random.randint supports \"Broadcasting\". \n",
    "        # Since 'maxval' (batch_counts) matches the requested shape (batch_size,),\n",
    "        # JAX will automatically use batch_counts[i] as the limit for item i.\n",
    "        time_indices = jax.random.randint(k2, (batch_size,), 0, batch_counts)\n",
    "\n",
    "        # 4. Gather the data\n",
    "        def get(arr): return arr[env_indices, time_indices]\n",
    "        \n",
    "        return Transition(\n",
    "            obs      = get(state.data.obs),\n",
    "            act      = get(state.data.act),\n",
    "            rew      = get(state.data.rew),\n",
    "            next_obs = get(state.data.next_obs),\n",
    "            done     = get(state.data.done)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a216d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import distrax\n",
    "from typing import Sequence, Callable\n",
    "\n",
    "# --- Helper: The Universal MLP Builder ---\n",
    "# Matches Spinning Up's 'mlp' function but handles NNX RNGs\n",
    "def build_mlp(\n",
    "    sizes: Sequence[int], \n",
    "    activation: Callable, \n",
    "    rngs: nnx.Rngs, \n",
    "    output_activation: Callable = None\n",
    "):\n",
    "    layers = []\n",
    "    for i in range(len(sizes) - 1):\n",
    "        # Add Linear Layer\n",
    "        layers.append(nnx.Linear(\n",
    "            sizes[i], sizes[i+1], \n",
    "            kernel_init=nnx.initializers.orthogonal(1.414),\n",
    "            bias_init=nnx.initializers.constant(0.0),\n",
    "            rngs=rngs\n",
    "            )\n",
    "        )\n",
    "        if i < len(sizes) - 2:\n",
    "            layers.append(activation)\n",
    "        elif output_activation is not None:\n",
    "            layers.append(output_activation)\n",
    "            \n",
    "    return nnx.Sequential(*layers)\n",
    "\n",
    "\n",
    "# --- 1. The Flexible Critic ---\n",
    "class Critic(nnx.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        obs_dim: int, \n",
    "        act_dim: int, \n",
    "        hidden_sizes: Sequence[int] = (256, 256), \n",
    "        activation: Callable = nnx.relu,\n",
    "        rngs: nnx.Rngs = None\n",
    "    ):\n",
    "        # Input to Q-net is Obs + Act\n",
    "        input_dim = obs_dim + act_dim\n",
    "        \n",
    "        # Full architecture: [Input, ...Hidden..., 1]\n",
    "        # We append [1] because Q-function outputs a single scalar value\n",
    "        layer_sizes = [input_dim] + list(hidden_sizes) + [1]\n",
    "        self.net1 = build_mlp(sizes=layer_sizes, activation=activation, rngs=rngs, output_activation=None)\n",
    "        self.net2 = build_mlp(sizes=layer_sizes, activation=activation, rngs=rngs, output_activation=None)\n",
    "\n",
    "    def __call__(self, obs, act):\n",
    "        x = jnp.concatenate([obs, act], axis=-1)\n",
    "        # Squeeze output to be (Batch,) instead of (Batch, 1)\n",
    "        return self.net1(x).squeeze(-1), self.net2(x).squeeze(-1)\n",
    "    \n",
    "\n",
    "class Actor(nnx.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes=(256, 256), rngs=None):\n",
    "        # 1. Base Network\n",
    "        self.net = build_mlp([obs_dim] + list(hidden_sizes), nnx.relu, rngs, nnx.relu)\n",
    "        \n",
    "        # 2. Heads\n",
    "        last_size = hidden_sizes[-1]\n",
    "        self.mu_layer = nnx.Linear(last_size, act_dim, \n",
    "                                   kernel_init=nnx.initializers.orthogonal(0.01), rngs=rngs)\n",
    "        self.log_std_layer = nnx.Linear(last_size, act_dim, \n",
    "                                        kernel_init=nnx.initializers.orthogonal(0.01), rngs=rngs)\n",
    "        self.act_limit = 1.0 # Standard for Brax/Gym\n",
    "\n",
    "    def __call__(self, obs):\n",
    "        \"\"\"Returns the Distribution object (used for Training Loss)\"\"\"\n",
    "        x = self.net(obs)\n",
    "        mu = self.mu_layer(x)\n",
    "        log_std = self.log_std_layer(x)\n",
    "        log_std = jnp.clip(log_std, -20, 2)\n",
    "        \n",
    "        base_dist = distrax.MultivariateNormalDiag(mu, jnp.exp(log_std))\n",
    "        return distrax.Transformed(base_dist, distrax.Block(distrax.Tanh(), ndims=1))\n",
    "\n",
    "    def get_deterministic_action(self, obs):\n",
    "        \"\"\"Used for Evaluation (No Noise)\"\"\"\n",
    "        x = self.net(obs)\n",
    "        mu = self.mu_layer(x)\n",
    "        # We just squash the mean directly. No sampling.\n",
    "        return jnp.tanh(mu) * self.act_limit\n",
    "\n",
    "    def get_stochastic_action(self, obs, key):\n",
    "        \"\"\"Used for Rollouts (With Noise)\"\"\"\n",
    "        dist = self(obs)\n",
    "        # Sample and scale (Distrax Tanh handles the squashing)\n",
    "        # We act_limit is usually 1.0, but good to keep explicit\n",
    "        return dist.sample(seed=key) * self.act_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1532838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "import optax\n",
    "\n",
    "# --- 1. The Actor Loss (Policy Optimization) ---\n",
    "def actor_loss_fn(actor: Actor, critic: Critic, batch: Transition, alpha, rng):\n",
    "    # 1. Sample actions from the CURRENT policy\n",
    "    # We need a key here because the policy is stochastic (Gaussian)\n",
    "    dist = actor(batch.obs)\n",
    "    pi_action, log_prob = dist.sample_and_log_prob(seed=rng)\n",
    "    \n",
    "    # 2. Get Q-values for these NEW actions\n",
    "    # Note: We use the 'critic' (not target) to grade the actor\n",
    "    q1, q2 = critic(batch.obs, pi_action)\n",
    "    min_q = jnp.minimum(q1, q2)\n",
    "    \n",
    "    # 3. SAC Objective: Maximize (MinQ - alpha * LogProb)\n",
    "    # Since we are minimizing loss, we flip the signs:\n",
    "    # Minimize: alpha * LogProb - MinQ\n",
    "    loss = (alpha * log_prob - min_q).mean()\n",
    "    \n",
    "    return loss, -log_prob.mean() # Return entropy for logging\n",
    "\n",
    "# --- 2. The Critic Loss (Q-Learning) ---\n",
    "def critic_loss_fn(critic: Critic, target_critic: Critic, actor: Actor, batch: Transition, alpha, rng):\n",
    "    # 1. Generate Target Actions (from Next State)\n",
    "    # We don't want gradients flowing through the target generation!\n",
    "    dist = actor(batch.next_obs)\n",
    "    next_action, next_log_prob = dist.sample_and_log_prob(seed=rng)\n",
    "    \n",
    "    # 2. Target Q-Values (Double Q-Learning)\n",
    "    # Use target_critic weights\n",
    "    target_q1, target_q2 = target_critic(batch.next_obs, next_action)\n",
    "    target_min_q = jnp.minimum(target_q1, target_q2)\n",
    "    \n",
    "    # 3. The Bellman Backup (Soft Q)\n",
    "    # Target = R + gamma * (1 - D) * (TargetQ - alpha * TargetEntropy)\n",
    "    gamma = 0.99\n",
    "    target_y = batch.rew + gamma * (1 - batch.done) * (target_min_q - alpha * next_log_prob)\n",
    "    \n",
    "    # 4. Current Q-Values\n",
    "    # We execute both heads on the current batch\n",
    "    current_q1, current_q2 = critic(batch.obs, batch.act)\n",
    "    \n",
    "    # 5. MSE Loss\n",
    "    loss_q1 = ((current_q1 - target_y) ** 2).mean()\n",
    "    loss_q2 = ((current_q2 - target_y) ** 2).mean()\n",
    "    \n",
    "    return loss_q1 + loss_q2, jnp.mean(target_min_q) # Log Q-vals\n",
    "\n",
    "# --- 3. The Update Step (Combining Everything) ---\n",
    "# This is the function that runs inside the Scan Loop\n",
    "def train_step(\n",
    "    actor: Actor, critic: Critic, target_critic: Critic,       # The Models\n",
    "    actor_opt: nnx.Optimizer, critic_opt: nnx.Optimizer,       # The Optimizers\n",
    "    batch: Transition,                                         # The Data\n",
    "    key,                                                       # The Master Key\n",
    "    alpha=0.2,                                                 # Fixed Entropy Temp (Simplified)\n",
    "    polyak=0.995                                               # Target Update Rate\n",
    "):\n",
    "    # A. Split Keys for the two stochastic operations\n",
    "    # key -> (next_key, key_for_actor_loss, key_for_critic_loss)\n",
    "    next_key, k1, k2 = jax.random.split(key, 3)\n",
    "    \n",
    "    # B. Update Critic\n",
    "    # nnx.value_and_grad gives us both the loss value (for logs) and gradients\n",
    "    (c_loss, c_log), c_grads = nnx.value_and_grad(critic_loss_fn, has_aux=True)(\n",
    "        critic, target_critic, actor, batch, alpha, k1\n",
    "    )\n",
    "    critic_opt.update(critic, c_grads)\n",
    "\n",
    "    # C. Update Actor\n",
    "    # Note: Actor update depends on Critic, so we do it second\n",
    "    (a_loss, entropy), a_grads = nnx.value_and_grad(actor_loss_fn, has_aux=True)(\n",
    "        actor, critic, batch, alpha, k2\n",
    "    )\n",
    "    actor_opt.update(actor, a_grads)\n",
    "    \n",
    "    # D. Update Target Networks (Polyak Averaging)\n",
    "    # Standard JAX tree map: new_target = polyak * target + (1-polyak) * source\n",
    "    # We access the parameters via 'nnx.state(model, nnx.Param)'\n",
    "    \n",
    "    # Helper to smooth weights\n",
    "    def soft_update(target_node, source_node):\n",
    "        return polyak * target_node + (1.0 - polyak) * source_node\n",
    "        \n",
    "    # We update the state of target_critic IN PLACE (conceptually)\n",
    "    # nnx.update performs the replacement safely\n",
    "    current_params = nnx.state(critic, nnx.Param)\n",
    "    target_params = nnx.state(target_critic, nnx.Param)\n",
    "    \n",
    "    new_target_params = jax.tree.map(soft_update, target_params, current_params)\n",
    "    nnx.update(target_critic, new_target_params)\n",
    "\n",
    "    # E. Return Logs and Key\n",
    "    metrics = {\n",
    "        \"loss_critic\": c_loss,\n",
    "        \"loss_actor\": a_loss,\n",
    "        \"q_val\": c_log,\n",
    "        \"entropy\": entropy\n",
    "    }\n",
    "    return next_key, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a69924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnagababa\u001b[0m (\u001b[33msbp_team\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/steven/Desktop/work/research/jax-learning/wandb/run-20260107_034839-uucph29v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sbp_team/jax-sac-playground/runs/uucph29v' target=\"_blank\">laced-sunset-3</a></strong> to <a href='https://wandb.ai/sbp_team/jax-sac-playground' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sbp_team/jax-sac-playground' target=\"_blank\">https://wandb.ai/sbp_team/jax-sac-playground</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sbp_team/jax-sac-playground/runs/uucph29v' target=\"_blank\">https://wandb.ai/sbp_team/jax-sac-playground/runs/uucph29v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/guides/checkpoint/api_refactor.html to migrate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Playground Env: CartpoleSwingup\n",
      "Obs: 5 | Act: 1 | Envs: 2048\n",
      "Initializing state...\n",
      "Warmup...\n",
      "Training...\n",
      "Epoch 0: Reward = 0.0322, Q-Val = 0.16\n",
      "Epoch 1: Reward = 0.0771, Q-Val = 0.93\n",
      "Epoch 2: Reward = 0.1122, Q-Val = 1.86\n",
      "Epoch 3: Reward = 0.1377, Q-Val = 2.90\n",
      "Epoch 4: Reward = 0.1691, Q-Val = 4.01\n",
      "Epoch 5: Reward = 0.2072, Q-Val = 5.20\n",
      "Epoch 6: Reward = 0.2601, Q-Val = 6.53\n",
      "Epoch 7: Reward = 0.2974, Q-Val = 7.98\n",
      "Epoch 8: Reward = 0.3192, Q-Val = 9.46\n",
      "Epoch 9: Reward = 0.3513, Q-Val = 10.95\n",
      "Epoch 10: Reward = 0.3906, Q-Val = 12.92\n",
      "Epoch 11: Reward = 0.4161, Q-Val = 15.24\n",
      "Epoch 12: Reward = 0.4476, Q-Val = 17.67\n",
      "Epoch 13: Reward = 0.4884, Q-Val = 20.25\n",
      "Epoch 14: Reward = 0.5111, Q-Val = 22.87\n",
      "Epoch 15: Reward = 0.5498, Q-Val = 25.66\n",
      "Epoch 16: Reward = 0.5794, Q-Val = 28.43\n",
      "Epoch 17: Reward = 0.6184, Q-Val = 31.31\n",
      "Epoch 18: Reward = 0.6339, Q-Val = 34.24\n",
      "Epoch 19: Reward = 0.6499, Q-Val = 37.25\n",
      "Epoch 20: Reward = 0.6464, Q-Val = 40.04\n",
      "Epoch 21: Reward = 0.6450, Q-Val = 42.70\n",
      "Epoch 22: Reward = 0.6587, Q-Val = 45.23\n",
      "Epoch 23: Reward = 0.7218, Q-Val = 47.79\n",
      "Epoch 24: Reward = 0.8077, Q-Val = 50.74\n",
      "Epoch 25: Reward = 0.8738, Q-Val = 54.13\n",
      "Epoch 26: Reward = 0.9018, Q-Val = 57.39\n",
      "Epoch 27: Reward = 0.9223, Q-Val = 60.75\n",
      "Epoch 28: Reward = 0.9337, Q-Val = 64.03\n",
      "Epoch 29: Reward = 0.9406, Q-Val = 67.25\n",
      "Epoch 30: Reward = 0.9431, Q-Val = 70.57\n",
      "Epoch 31: Reward = 0.9447, Q-Val = 73.92\n",
      "Epoch 32: Reward = 0.9451, Q-Val = 77.25\n",
      "Epoch 33: Reward = 0.9456, Q-Val = 80.28\n",
      "Epoch 34: Reward = 0.9455, Q-Val = 82.69\n",
      "Epoch 35: Reward = 0.9457, Q-Val = 84.44\n",
      "Epoch 36: Reward = 0.9456, Q-Val = 85.83\n",
      "Epoch 37: Reward = 0.9457, Q-Val = 87.02\n",
      "Epoch 38: Reward = 0.9457, Q-Val = 88.05\n",
      "Epoch 39: Reward = 0.9453, Q-Val = 88.97\n",
      "Epoch 40: Reward = 0.9455, Q-Val = 89.82\n",
      "Epoch 41: Reward = 0.9454, Q-Val = 90.62\n",
      "Epoch 42: Reward = 0.9452, Q-Val = 91.37\n",
      "Epoch 43: Reward = 0.9452, Q-Val = 92.08\n",
      "Epoch 44: Reward = 0.9453, Q-Val = 92.75\n",
      "Epoch 45: Reward = 0.9450, Q-Val = 93.39\n",
      "Epoch 46: Reward = 0.9449, Q-Val = 94.00\n",
      "Epoch 47: Reward = 0.9450, Q-Val = 94.57\n",
      "Epoch 48: Reward = 0.9450, Q-Val = 95.12\n",
      "Epoch 49: Reward = 0.9447, Q-Val = 95.64\n",
      "Saving checkpoint step 50000...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "import wandb\n",
    "from collections import namedtuple\n",
    "\n",
    "# Playground Imports\n",
    "from mujoco_playground import registry\n",
    "from brax.envs import Wrapper, State\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "os.environ['JAX_DEFAULT_MATMUL_PRECISION'] = 'highest'\n",
    "\n",
    "ENV_NAME = 'CartpoleSwingup'\n",
    "NUM_ENVS = 2048\n",
    "TOTAL_STEPS = 50_000\n",
    "BATCH_SIZE = 256\n",
    "HIDDEN_SIZES = (256, 256)\n",
    "LR = 3e-4\n",
    "ALPHA = 0.2\n",
    "WARMUP_STEPS = 100\n",
    "LOG_EVERY = 10\n",
    "\n",
    "# --- 2. ADAPTERS & HELPERS ---\n",
    "_PseudoStateBase = namedtuple('PseudoState', ['data', 'obs', 'reward', 'done', 'metrics', 'info'])\n",
    "\n",
    "class PseudoState(_PseudoStateBase):\n",
    "    def replace(self, **kwargs):\n",
    "        return self._replace(**kwargs)\n",
    "\n",
    "class BraxAdapter(Wrapper):\n",
    "    def reset(self, rng):\n",
    "        state = self.env.reset(rng)\n",
    "        return self._to_brax_state(state)\n",
    "\n",
    "    def step(self, state, action):\n",
    "        mock_state = PseudoState(\n",
    "            data=state.pipeline_state,\n",
    "            obs=state.obs,\n",
    "            reward=state.reward,\n",
    "            done=state.done,\n",
    "            metrics=state.metrics,\n",
    "            info=state.info\n",
    "        )\n",
    "        next_state = self.env.step(mock_state, action)\n",
    "        return self._to_brax_state(next_state)\n",
    "\n",
    "    def _to_brax_state(self, play_state):\n",
    "        return State(\n",
    "            pipeline_state=play_state.data,\n",
    "            obs=play_state.obs,\n",
    "            reward=play_state.reward,\n",
    "            done=play_state.done,\n",
    "            metrics=play_state.metrics,\n",
    "            info=play_state.info\n",
    "        )\n",
    "\n",
    "# --- 3. THE CONTAINER (To keep scan clean) ---\n",
    "# We use this to bundle everything that changes during training\n",
    "class AgentState(nnx.Variable):\n",
    "    # This is just a dummy class to hold references if we wanted\n",
    "    # But simpler is just a Python Dataclass or Tuple.\n",
    "    # Let's use a simple dictionary-like structure for JAX to carry.\n",
    "    pass\n",
    "\n",
    "import orbax.checkpoint as ocp\n",
    "# --- 3.5 CHECKPOINTING HELPERS ---\n",
    "def create_checkpoint_manager(base_dir, max_to_keep=3):\n",
    "    abs_path = os.path.abspath(base_dir)\n",
    "    options = ocp.CheckpointManagerOptions(max_to_keep=max_to_keep, create=True)\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    return ocp.CheckpointManager(abs_path, checkpointer, options=options)\n",
    "\n",
    "def save_ckpt(manager, step, actor, critic, target_critic, actor_opt, critic_opt):\n",
    "    # Extract state from NNX modules\n",
    "    payload = {\n",
    "        'actor': nnx.state(actor),\n",
    "        'critic': nnx.state(critic),\n",
    "        'target_critic': nnx.state(target_critic),\n",
    "        'actor_opt': nnx.state(actor_opt),\n",
    "        'critic_opt': nnx.state(critic_opt),\n",
    "    }\n",
    "    # Save (blocking)\n",
    "    print(f\"Saving checkpoint step {step}...\")\n",
    "    manager.save(step, payload)\n",
    "\n",
    "def restore_ckpt(manager, actor, critic, target_critic, actor_opt, critic_opt):\n",
    "    latest_step = manager.latest_step()\n",
    "    if latest_step is None:\n",
    "        print(\"No checkpoint found. Starting fresh.\")\n",
    "        return 0\n",
    "\n",
    "    print(f\"Restoring from step {latest_step}...\")\n",
    "    \n",
    "    # Create structure template\n",
    "    target_payload = {\n",
    "        'actor': nnx.state(actor),\n",
    "        'critic': nnx.state(critic),\n",
    "        'target_critic': nnx.state(target_critic),\n",
    "        'actor_opt': nnx.state(actor_opt),\n",
    "        'critic_opt': nnx.state(critic_opt),\n",
    "    }\n",
    "    \n",
    "    restored = manager.restore(latest_step, items=target_payload)\n",
    "    \n",
    "    # Update objects in-place\n",
    "    nnx.update(actor, restored['actor'])\n",
    "    nnx.update(critic, restored['critic'])\n",
    "    nnx.update(target_critic, restored['target_critic'])\n",
    "    nnx.update(actor_opt, restored['actor_opt'])\n",
    "    nnx.update(critic_opt, restored['critic_opt'])\n",
    "    \n",
    "    return latest_step\n",
    "\n",
    "\n",
    "# --- 4. MAIN LOOP ---\n",
    "def main():\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"jax-sac-playground\",\n",
    "        config={\n",
    "            \"env\": ENV_NAME,\n",
    "            \"num_envs\": NUM_ENVS,\n",
    "            \"total_steps\": TOTAL_STEPS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"lr\": LR,\n",
    "            \"hidden_sizes\": HIDDEN_SIZES\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ckpt_dir = \"./sac_checkpoints\"\n",
    "    ckpt_manager = create_checkpoint_manager(ckpt_dir)\n",
    "\n",
    "    print(f\"Loading Playground Env: {ENV_NAME}\")\n",
    "    base_env = registry.load(ENV_NAME)\n",
    "    env = BraxAdapter(base_env)\n",
    "    \n",
    "    jit_reset = jax.jit(jax.vmap(env.reset))\n",
    "    jit_step = jax.jit(jax.vmap(env.step))\n",
    "    \n",
    "    obs_dim = env.observation_size\n",
    "    act_dim = env.action_size\n",
    "    print(f\"Obs: {obs_dim} | Act: {act_dim} | Envs: {NUM_ENVS}\")\n",
    "\n",
    "    # --- INITIALIZATION ---\n",
    "    key = nnx.Rngs(0)\n",
    "    actor = Actor(obs_dim, act_dim, HIDDEN_SIZES, rngs=key)\n",
    "    critic = Critic(obs_dim, act_dim, HIDDEN_SIZES, rngs=key)\n",
    "    target_critic = Critic(obs_dim, act_dim, HIDDEN_SIZES, rngs=nnx.Rngs(0))\n",
    "\n",
    "    actor_opt = nnx.Optimizer(actor, optax.adam(LR), wrt=nnx.Param)\n",
    "    critic_opt = nnx.Optimizer(critic, optax.adam(LR), wrt=nnx.Param)\n",
    "\n",
    "    buffer = BatchedReplayBuffer(NUM_ENVS, capacity=10_000, obs_dim=obs_dim, act_dim=act_dim)\n",
    "    buffer_state = buffer.init()\n",
    "\n",
    "    # --- 5. ROLLOUT STEP (Fixed Signature) ---\n",
    "    # We pass the models IN through the carry arguments\n",
    "    def rollout_step(carry, _):\n",
    "        # UNPACK EVERYTHING\n",
    "        (env_state, buf_state, key, \n",
    "         actor, critic, target_critic, \n",
    "         actor_opt, critic_opt) = carry\n",
    "        \n",
    "        # 1. Action\n",
    "        key, act_key = jax.random.split(key)\n",
    "        act_keys = jax.random.split(act_key, NUM_ENVS)\n",
    "        # Actor is now a \"Tracer\" object local to this loop, so we can use it safely\n",
    "        action = jax.vmap(actor.get_stochastic_action)(env_state.obs, act_keys)\n",
    "        \n",
    "        # 2. Step\n",
    "        next_env_state = jit_step(env_state, action)\n",
    "        real_next_obs = next_env_state.obs\n",
    "        \n",
    "        trans = Transition(\n",
    "            obs=env_state.obs,\n",
    "            act=action,\n",
    "            rew=next_env_state.reward,\n",
    "            next_obs=real_next_obs,\n",
    "            done=next_env_state.done\n",
    "        )\n",
    "        buf_state = buffer.add(buf_state, trans)\n",
    "        \n",
    "        # 3. Train\n",
    "        batch = buffer.sample(buf_state, key, BATCH_SIZE)\n",
    "        \n",
    "        # This function updates the local 'actor/critic' variables in-place\n",
    "        key, train_metrics = train_step(\n",
    "            actor, critic, target_critic, \n",
    "            actor_opt, critic_opt, \n",
    "            batch, key, ALPHA\n",
    "        )\n",
    "\n",
    "        avg_reward = next_env_state.reward.mean()\n",
    "\n",
    "        metrics = {\n",
    "            **train_metrics,          # q_val, loss_actor, etc.\n",
    "            \"env_reward\": avg_reward, # The raw reward signal\n",
    "            \"episode_return\": env_state.metrics.get('episode_return', 0.0) # If playground provides it\n",
    "        }\n",
    "        \n",
    "        # REPACK EVERYTHING\n",
    "        # We must return the modified actor/critic objects\n",
    "        new_carry = (next_env_state, buf_state, key, \n",
    "                     actor, critic, target_critic, \n",
    "                     actor_opt, critic_opt)\n",
    "        \n",
    "        return new_carry, metrics\n",
    "\n",
    "    # --- 6. EXECUTION ---\n",
    "    print(\"Initializing state...\")\n",
    "    master_key = jax.random.PRNGKey(42)\n",
    "    master_key, reset_key = jax.random.split(master_key)\n",
    "    reset_keys = jax.random.split(reset_key, NUM_ENVS)\n",
    "    env_state = jit_reset(reset_keys)\n",
    "\n",
    "    # Warmup\n",
    "    print(\"Warmup...\")\n",
    "    def warmup_fn(carry, _):\n",
    "        es, bs, k = carry\n",
    "        k, ak = jax.random.split(k)\n",
    "        action = jax.random.uniform(ak, (NUM_ENVS, act_dim), minval=-1, maxval=1)\n",
    "        nes = jit_step(es, action)\n",
    "        trans = Transition(es.obs, action, nes.reward, nes.obs, nes.done)\n",
    "        bs = buffer.add(bs, trans)\n",
    "        return (nes, bs, k), None\n",
    "        \n",
    "    (env_state, buffer_state, master_key), _ = jax.lax.scan(\n",
    "        warmup_fn, (env_state, buffer_state, master_key), None, length=WARMUP_STEPS\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    print(\"Training...\")\n",
    "    steps_per_epoch = 1000\n",
    "    num_epochs = TOTAL_STEPS // steps_per_epoch\n",
    "    \n",
    "    # We construct the MEGA CARRY tuple\n",
    "    carry = (env_state, buffer_state, master_key, \n",
    "             actor, critic, target_critic, \n",
    "             actor_opt, critic_opt)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # scan returns the new tuple with updated models\n",
    "        carry, metrics_history = jax.lax.scan(\n",
    "            rollout_step, \n",
    "            carry, \n",
    "            None, \n",
    "            length=steps_per_epoch\n",
    "        )\n",
    "        \n",
    "        # <--- 4. AGGREGATE & LOG\n",
    "        # We average the 1000 steps to get one data point for WandB\n",
    "        # jax.tree.map applies jnp.mean to every item in the dict\n",
    "        avg_metrics = jax.tree.map(lambda x: jnp.mean(x), metrics_history)\n",
    "        \n",
    "        # Convert JAX arrays to standard Python floats for WandB\n",
    "        log_dict = {k: float(v) for k, v in avg_metrics.items()}\n",
    "        log_dict[\"step\"] = (epoch + 1) * steps_per_epoch\n",
    "        \n",
    "        wandb.log(log_dict)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Reward = {log_dict['env_reward']:.4f}, Q-Val = {log_dict['q_val']:.2f}\")\n",
    "\n",
    "        # Unpack the current models from carry to save them\n",
    "        current_step = (epoch + 1) * steps_per_epoch\n",
    "        \n",
    "        # Note: carry[3] is actor, carry[4] is critic, etc. based on your tuple\n",
    "        (_, _, _, cur_actor, cur_critic, cur_target, cur_act_opt, cur_crit_opt) = carry\n",
    "        \n",
    "        # Save every 5 epochs (or whatever frequency you prefer)\n",
    "        if (epoch + 1) % LOG_EVERY == 0:\n",
    "            print(\"logging\")\n",
    "            save_ckpt(\n",
    "                ckpt_manager, \n",
    "                current_step, \n",
    "                cur_actor, cur_critic, cur_target, cur_act_opt, cur_crit_opt\n",
    "            )\n",
    "\n",
    "    (_, _, _, cur_actor, cur_critic, cur_target, cur_act_opt, cur_crit_opt) = carry\n",
    "    save_ckpt(\n",
    "        ckpt_manager, \n",
    "        TOTAL_STEPS, \n",
    "        cur_actor, cur_critic, cur_target, cur_act_opt, cur_crit_opt\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14d5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/guides/checkpoint/api_refactor.html to migrate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Env: CartpoleSwingup\n",
      "Restoring from step 50000...\n",
      "Restored checkpoint from step None.\n",
      "Simulating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/subprocess.py:1885: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _fork_exec(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to: CartpoleSwingup_sac.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "import mujoco\n",
    "import numpy as np\n",
    "import imageio\n",
    "import orbax.checkpoint as ocp\n",
    "from collections import namedtuple\n",
    "from mujoco_playground import registry\n",
    "from brax.envs import Wrapper, State\n",
    "import absl.logging\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "ENV_NAME = 'CartpoleSwingup'\n",
    "CHECKPOINT_DIR = \"./sac_checkpoints\" # Must match your training checkpoint directory\n",
    "HIDDEN_SIZES = (256, 256) # Must match training\n",
    "LR = 3e-4\n",
    "\n",
    "# --- 3. ADAPTERS (Standard) ---\n",
    "_PseudoStateBase = namedtuple('PseudoState', ['data', 'obs', 'reward', 'done', 'metrics', 'info'])\n",
    "class PseudoState(_PseudoStateBase):\n",
    "    def replace(self, **kwargs): return self._replace(**kwargs)\n",
    "\n",
    "class BraxAdapter(Wrapper):\n",
    "    def reset(self, rng):\n",
    "        state = self.env.reset(rng)\n",
    "        return self._to_brax_state(state)\n",
    "    def step(self, state, action):\n",
    "        mock_state = PseudoState(\n",
    "            data=state.pipeline_state,\n",
    "            obs=state.obs,\n",
    "            reward=state.reward,\n",
    "            done=state.done,\n",
    "            metrics=state.metrics,\n",
    "            info=state.info\n",
    "        )\n",
    "        next_state = self.env.step(mock_state, action)\n",
    "        return self._to_brax_state(next_state)\n",
    "    def _to_brax_state(self, s):\n",
    "        return State(pipeline_state=s.data, obs=s.obs, reward=s.reward, done=s.done, metrics=s.metrics, info=s.info)\n",
    "\n",
    "# --- 4. CHECKPOINT LOADER ---\n",
    "# We reuse the training helpers `create_checkpoint_manager` and `restore_ckpt`\n",
    "# defined in the previous cell.\n",
    "\n",
    "import orbax.checkpoint as ocp\n",
    "# --- 3.5 CHECKPOINTING HELPERS ---\n",
    "def create_checkpoint_manager(base_dir, max_to_keep=3):\n",
    "    abs_path = os.path.abspath(base_dir)\n",
    "    options = ocp.CheckpointManagerOptions(max_to_keep=max_to_keep, create=True)\n",
    "    checkpointer = ocp.StandardCheckpointer()\n",
    "    return ocp.CheckpointManager(abs_path, checkpointer, options=options)\n",
    "\n",
    "\n",
    "def restore_ckpt(manager, actor, critic, target_critic, actor_opt, critic_opt):\n",
    "    latest_step = manager.latest_step()\n",
    "    if latest_step is None:\n",
    "        print(\"No checkpoint found. Starting fresh.\")\n",
    "        return 0\n",
    "\n",
    "    print(f\"Restoring from step {latest_step}...\")\n",
    "    \n",
    "    # Create structure template\n",
    "    target_payload = {\n",
    "        'actor': nnx.state(actor),\n",
    "        'critic': nnx.state(critic),\n",
    "        'target_critic': nnx.state(target_critic),\n",
    "        'actor_opt': nnx.state(actor_opt),\n",
    "        'critic_opt': nnx.state(critic_opt),\n",
    "    }\n",
    "    \n",
    "    restored = manager.restore(latest_step, items=target_payload)\n",
    "    \n",
    "    # Update objects in-place\n",
    "    nnx.update(actor, restored['actor'])\n",
    "    nnx.update(critic, restored['critic'])\n",
    "    nnx.update(target_critic, restored['target_critic'])\n",
    "    nnx.update(actor_opt, restored['actor_opt'])\n",
    "    nnx.update(critic_opt, restored['critic_opt'])\n",
    "\n",
    "def main():\n",
    "    print(f\"Initializing Env: {ENV_NAME}\")\n",
    "    raw_env = registry.load(ENV_NAME)\n",
    "    env = BraxAdapter(raw_env)\n",
    "    obs_dim = env.observation_size\n",
    "    act_dim = env.action_size\n",
    "\n",
    "    # --- INIT ACTOR & CRITICS, OPTIMIZERS ---\n",
    "    key = nnx.Rngs(0)\n",
    "    actor = Actor(obs_dim, act_dim, HIDDEN_SIZES, rngs=key)\n",
    "    critic = Critic(obs_dim, act_dim, HIDDEN_SIZES, rngs=key)\n",
    "    target_critic = Critic(obs_dim, act_dim, HIDDEN_SIZES, rngs=nnx.Rngs(0))\n",
    "\n",
    "    actor_opt = nnx.Optimizer(actor, optax.adam(LR), wrt=nnx.Param)\n",
    "    critic_opt = nnx.Optimizer(critic, optax.adam(LR), wrt=nnx.Param)\n",
    "\n",
    "    # --- SETUP CHECKPOINT MANAGER (reuse training helper) ---\n",
    "    ckpt_manager = create_checkpoint_manager(CHECKPOINT_DIR)\n",
    "    latest_step = restore_ckpt(ckpt_manager, actor, critic, target_critic, actor_opt, critic_opt)\n",
    "    if latest_step == 0:\n",
    "        raise ValueError(f\"No checkpoints found in {os.path.abspath(CHECKPOINT_DIR)}\")\n",
    "    print(f\"Restored checkpoint from step {latest_step}.\")\n",
    "\n",
    "    # --- SETUP RENDERING ---\n",
    "    mj_model = raw_env.mj_model\n",
    "    mj_data = mujoco.MjData(mj_model)\n",
    "    renderer = mujoco.Renderer(mj_model, height=480, width=640)\n",
    "\n",
    "    jit_reset = jax.jit(env.reset)\n",
    "    jit_step = jax.jit(env.step)\n",
    "\n",
    "    @jax.jit\n",
    "    def get_action(obs):\n",
    "        obs_batched = obs[None, :] \n",
    "        action_batched = actor.get_deterministic_action(obs_batched)\n",
    "        return action_batched[0]\n",
    "\n",
    "    print(\"Simulating...\")\n",
    "    frames = []\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    state = jit_reset(rng)\n",
    "\n",
    "    for i in range(500):\n",
    "        action = get_action(state.obs)\n",
    "        state = jit_step(state, action)\n",
    "        mj_data.qpos = np.array(state.pipeline_state.qpos)\n",
    "        mj_data.qvel = np.array(state.pipeline_state.qvel)\n",
    "        mujoco.mj_forward(mj_model, mj_data)\n",
    "        renderer.update_scene(mj_data)\n",
    "        frames.append(renderer.render())\n",
    "        if state.done:\n",
    "            print(f\"Episode ended at step {i}\")\n",
    "            # Optional: state = jit_reset(rng)\n",
    "\n",
    "    output_name = f'{ENV_NAME}_sac.mp4'\n",
    "    imageio.mimsave(output_name, frames, fps=60)\n",
    "    print(f\"Video saved to: {output_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
